{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第14章の章末演習問題\n",
    "※ ここではGoogle Colaraboratoryでの実行を想定しています。\n",
    "\n",
    "※ Google Colaraboratoryでbashコマンドを実行するには、命令の前に!をつけます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 分類用のテストセットを実装するか、第13章の演習のテストセットを再利用してください。検証セットを使用して訓練中に最適なエポックを選択しますが、テストセットを使用してエンドツーエンドのプロジェクトを評価します。この時、 検証セットの性能は、テストセットの性能とどの程度一致していますか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストセットの性能は検証セットと比較して悪くなります。（※ テストと検証の分け方にも依存します） \\\n",
    "今回使用したデータは、アノテーションデータが少なく、訓練・検証・テストの3つに分けるとどうしても訓練データには存在しないデータがテストセットに含まれる可能性が高くなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] 非結節、良性結節、悪性腫瘍を一度に区別して三分類ができる単一のモデルを訓練できますか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解ラベルを非結節、良性結節、悪性腫瘍の3つとした分類モデルを作成することで可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) 訓練に最適なクラスバランスの分け方はどうなりますか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各クラスの学習セットがだいたい同じになるように調節します。 \\\n",
    "何も行わない場合、非結節の学習データが多すぎるため、正しく分類が行われない可能性が高いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) このように一度に分類を行うモデルは、本書の中で使用している2段階のアプローチと比較して、どのような性能を発揮するのでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回のような不均衡なデータ（正解データが少ない）場合、一度に分類を行うと精度が下がります。 \\\n",
    "一般的に分類するクラス数が増加すると、学習の難易度は上昇します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] 今回はアノテーション付きのデータで分類器を訓練しましたが、セグメンテーションの出力でそれが実行されることを期待しています。セグメンテーションモデルを使用して、提供された非結節の代わりに、トレーニング中に使用する非結節のリストを作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) この新しいセットで訓練すると、分類モデルの性能は向上しますか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.11.1-py3-none-any.whl (285 kB)\n",
      "\u001b[K     |████████████████████████████████| 285 kB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.23 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from seaborn) (0.25.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from seaborn) (1.5.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from seaborn) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from seaborn) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from pandas>=0.23->seaborn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from matplotlib>=2.2->seaborn) (2020.12.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from matplotlib>=2.2->seaborn) (6.2.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from matplotlib>=2.2->seaborn) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas>=0.23->seaborn) (1.15.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from operator import attrgetter\n",
    "import scipy.ndimage.measurements as measurements\n",
    "import scipy.ndimage.morphology as morphology\n",
    "from p2ch14_exercise3.model13 import UNetWrapper, SegmentationAugmentation\n",
    "from p2ch14_exercise3.dsets13 import getCandidateInfoList, getCt, TrainingLuna2dSegmentationDataset, CandidateInfoTuple, Luna2dSegmentationDataset\n",
    "from p2ch14_exercise3.dsets import LunaDataset\n",
    "from p2ch14_exercise3.model import LunaModel\n",
    "from util.util import xyz2irc, irc2xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-unversioned/part2/luna配下に存在するデータのみを今回の処理の対象にする。\n",
    "series_uid = glob.glob('data-unversioned/part2/luna/subset0/*.mhd')\n",
    "series_uid = list(map(lambda x: x[x.rfind('/')+1:-4], series_uid))\n",
    "segmentation_train_series_uid = series_uid[0:15]\n",
    "segmentation_valid_series_uid = [series_uid[15:20]]\n",
    "classification_train_series_uid = series_uid[20:35]\n",
    "classification_valid_series_uid = series_uid[35:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_SIZE=10\n",
    "METRICS_LOSS_NDX = 1\n",
    "METRICS_TP_NDX = 7\n",
    "METRICS_FN_NDX = 8\n",
    "METRICS_FP_NDX = 9\n",
    "\n",
    "def getDataloader(series_uid:str)->torch.utils.data.DataLoader:\n",
    "    ds = TrainingLuna2dSegmentationDataset(\n",
    "            val_stride=0,\n",
    "            isValSet_bool=False,\n",
    "            contextSlices_count=3,\n",
    "            series_uid=series_uid\n",
    "    )\n",
    "    return  torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=2,\n",
    "        pin_memory=False)\n",
    "    \n",
    "def doTraining(model:torch.nn.Module, optimizer:torch.optim.Optimizer, epoch_ndx:int, train_dl:torch.utils.data.DataLoader)->torch.Tensor:\n",
    "        trnMetrics_g = torch.zeros(METRICS_SIZE, len(train_dl.dataset), device='cuda')\n",
    "        model.train()\n",
    "        train_dl.dataset.shuffleSamples()\n",
    "        bar = tqdm(train_dl)\n",
    "        for batch_ndx, batch_tup in enumerate(bar):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_var = computeBatchLoss(model, batch_ndx, batch_tup, train_dl.batch_size, trnMetrics_g)\n",
    "            loss_var.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            bar.set_description('loss: %.5f' % loss_var.item())\n",
    "\n",
    "        return trnMetrics_g.to('cpu')\n",
    "\n",
    "def doValidation(model:torch.nn.Module, epoch_ndx, val_dl)->torch.Tensor:\n",
    "    bar = tqdm(val_dl)\n",
    "    with torch.no_grad():\n",
    "        valMetrics_g = torch.zeros(METRICS_SIZE, len(val_dl.dataset), device='cuda')\n",
    "        model.eval()\n",
    "\n",
    "        for batch_ndx, batch_tup in enumerate(bar):\n",
    "            computeBatchLoss(model, batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g)\n",
    "\n",
    "    return valMetrics_g.to('cpu')\n",
    "\n",
    "def computeBatchLoss(segmentation_model:torch.nn.Module, batch_ndx:int, batch_tup:tuple, batch_size:int, metrics_g:torch.Tensor,\n",
    "                        classificationThreshold:float=0.5)->torch.Tensor:\n",
    "    input_t, label_t, series_list, _slice_ndx_list = batch_tup\n",
    "\n",
    "    input_g = input_t.to('cuda', non_blocking=True)\n",
    "    label_g = label_t.to('cuda', non_blocking=True)\n",
    "\n",
    "    prediction_g = segmentation_model(input_g)\n",
    "\n",
    "    diceLoss_g = diceLoss(prediction_g, label_g)\n",
    "    fnLoss_g = diceLoss(prediction_g * label_g, label_g)\n",
    "\n",
    "    start_ndx = batch_ndx * batch_size\n",
    "    end_ndx = start_ndx + input_t.size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictionBool_g = (prediction_g[:, 0:1]\n",
    "                            > classificationThreshold).to(torch.float32)\n",
    "\n",
    "        tp = (     predictionBool_g *  label_g).sum(dim=[1,2,3])\n",
    "        fn = ((1 - predictionBool_g) *  label_g).sum(dim=[1,2,3])\n",
    "        fp = (     predictionBool_g * (~label_g)).sum(dim=[1,2,3])\n",
    "\n",
    "        metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n",
    "        metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
    "        metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
    "        metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
    "\n",
    "    return diceLoss_g.mean() + fnLoss_g.mean() * 8\n",
    "\n",
    "def diceLoss(prediction_g, label_g, epsilon=1):\n",
    "    diceLabel_g = label_g.sum(dim=[1,2,3])\n",
    "    dicePrediction_g = prediction_g.sum(dim=[1,2,3])\n",
    "    diceCorrect_g = (prediction_g * label_g).sum(dim=[1,2,3])\n",
    "\n",
    "    diceRatio_g = (2 * diceCorrect_g + epsilon) / (dicePrediction_g + diceLabel_g + epsilon)\n",
    "\n",
    "    return 1 - diceRatio_g\n",
    "\n",
    "def logMetrics(epoch_ndx, mode_str, metrics_t):\n",
    "\n",
    "    metrics_a = metrics_t.detach().numpy()\n",
    "    sum_a = metrics_a.sum(axis=1)\n",
    "    assert np.isfinite(metrics_a).all()\n",
    "\n",
    "    allLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['loss/all'] = metrics_a[METRICS_LOSS_NDX].mean()\n",
    "    metrics_dict['percent_all/tp'] = sum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100\n",
    "    metrics_dict['percent_all/fn'] = sum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100\n",
    "    metrics_dict['percent_all/fp'] = sum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100\n",
    "\n",
    "\n",
    "    precision = metrics_dict['pr/precision'] = sum_a[METRICS_TP_NDX] / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FP_NDX]) or 1)\n",
    "    recall    = metrics_dict['pr/recall']    = sum_a[METRICS_TP_NDX] / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]) or 1)\n",
    "\n",
    "    metrics_dict['pr/f1_score'] = 2 * (precision * recall) / ((precision + recall) or 1)\n",
    "    return metrics_dict['pr/recall']\n",
    "\n",
    "def groupSegmentationOutput(series_uid,  ct, clean_a):\n",
    "    candidateLabel_a, candidate_count = measurements.label(clean_a)\n",
    "    centerIrc_list = measurements.center_of_mass(\n",
    "        ct.hu_a.clip(-1000, 1000) + 1001,\n",
    "        labels=candidateLabel_a,\n",
    "        index=np.arange(1, candidate_count+1),\n",
    "    )\n",
    "    candidateInfo_list = []\n",
    "    for i, center_irc in enumerate(centerIrc_list):\n",
    "        center_xyz = irc2xyz(\n",
    "            center_irc,\n",
    "            ct.origin_xyz,\n",
    "            ct.vxSize_xyz,\n",
    "            ct.direction_a,\n",
    "        )\n",
    "        assert np.all(np.isfinite(center_irc)), repr(['irc', center_irc, i, candidate_count])\n",
    "        assert np.all(np.isfinite(center_xyz)), repr(['xyz', center_xyz])\n",
    "        candidateInfo_tup = CandidateInfoTuple(False, False, False, 0.0, series_uid, center_xyz)\n",
    "        candidateInfo_list.append(candidateInfo_tup)\n",
    "\n",
    "    return candidateInfo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSegmentationDl(series_uid):\n",
    "    seg_ds = Luna2dSegmentationDataset(\n",
    "            contextSlices_count=3,\n",
    "            series_uid=series_uid,\n",
    "            fullCt_bool=True,\n",
    "        )\n",
    "    seg_dl = torch.utils.data.DataLoader(\n",
    "        seg_ds,\n",
    "        batch_size=BATCH_SIZE * torch.cuda.device_count(),\n",
    "        num_workers=2,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return seg_dl\n",
    "\n",
    "def segmentCt(seg_model:torch.nn.Module, ct, series_uid):\n",
    "    with torch.no_grad():\n",
    "        output_a = np.zeros_like(ct.hu_a, dtype=np.float32)\n",
    "        seg_dl = initSegmentationDl([series_uid])  #  <3>\n",
    "        for input_t, _, _, slice_ndx_list in seg_dl:\n",
    "\n",
    "            input_g = input_t.to('cuda')            \n",
    "            prediction_g = seg_model(input_g)\n",
    "\n",
    "            for i, slice_ndx in enumerate(slice_ndx_list):\n",
    "                output_a[slice_ndx] = prediction_g[i].cpu().numpy()\n",
    "\n",
    "        mask_a = output_a < 0.5\n",
    "        mask_a = morphology.binary_erosion(mask_a, iterations=1)\n",
    "\n",
    "    return mask_a\n",
    "\n",
    "def getSegmentedCandidateList(segmentation_model:torch.nn.Module, series_uid:str)->list:\n",
    "    candidate_info_list = []\n",
    "    for uid in series_uid:\n",
    "        ct = getCt(uid)\n",
    "        mask_a = segmentCt(segmentation_model, ct, uid)\n",
    "        candidate_info_list += groupSegmentationOutput(uid, ct, mask_a)\n",
    "    return candidate_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_STOPPING = 3\n",
    "N_EPOCHS = 20\n",
    "BATCH_SIZE=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-01-27 05:50:21,760 INFO     pid:27168 p2ch14_exercise3.dsets13:305:__init__ <p2ch14_exercise3.dsets13.TrainingLuna2dSegmentationDataset object at 0x7f3151322518>: 15 training series, 156 slices, 18 nodules\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f569175990e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_uid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegmentation_train_series_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalid_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetDataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_uid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegmentation_valid_series_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m segmentation_model = UNetWrapper(\n\u001b[1;32m      4\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-3b1d284d1617>\u001b[0m in \u001b[0;36mgetDataloader\u001b[0;34m(series_uid)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0misValSet_bool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mcontextSlices_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mseries_uid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseries_uid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     return  torch.utils.data.DataLoader(\n",
      "\u001b[0;32m~/work/deep-learning-with-pytorch-ja/p2ch14_exercise3/dsets13.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTrainingLuna2dSegmentationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLuna2dSegmentationDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/deep-learning-with-pytorch-ja/p2ch14_exercise3/dsets13.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, val_stride, isValSet_bool, series_uid, contextSlices_count, fullCt_bool)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mseries_uid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mindex_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetCtSampleSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfullCt_bool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36_pytorch/lib/python3.6/site-packages/diskcache/core.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1888\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mENOVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1889\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1890\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mexpire\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mexpire\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpire\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/deep-learning-with-pytorch-ja/p2ch14_exercise3/dsets13.py\u001b[0m in \u001b[0;36mgetCtSampleSize\u001b[0;34m(series_uid)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mraw_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtyped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetCtSampleSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhu_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositive_indexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/deep-learning-with-pytorch-ja/p2ch14_exercise3/dsets13.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, series_uid)\u001b[0m\n\u001b[1;32m    105\u001b[0m         mhd_path = glob.glob(\n\u001b[1;32m    106\u001b[0m             \u001b[0;34m\"data-unversioned/part2/luna/subset*/{}.mhd\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_uid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         )[0]\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mct_mhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msitk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReadImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmhd_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "train_dl = getDataloader(series_uid=segmentation_train_series_uid)\n",
    "valid_dl = getDataloader(series_uid=segmentation_valid_series_uid)\n",
    "segmentation_model = UNetWrapper(\n",
    "            in_channels=7,\n",
    "            n_classes=1,\n",
    "            depth=3,\n",
    "            wf=4,\n",
    "            padding=True,\n",
    "            batch_norm=True,\n",
    "            up_mode='upconv',\n",
    "        )\n",
    "segmentation_model=segmentation_model.to('cuda')\n",
    "\n",
    "best_score = 0.0\n",
    "train_scores=[]\n",
    "valid_scores=[]\n",
    "optimizer = torch.optim.Adam(segmentation_model.parameters())\n",
    "early_stopping_count = 0\n",
    "for epoch_ndx in range(1, N_EPOCHS+1):\n",
    "    trnMetrics_t = doTraining(segmentation_model, optimizer, epoch_ndx, train_dl)\n",
    "    valMetrics_t = doValidation(segmentation_model, epoch_ndx, valid_dl)\n",
    "    train_scores.append(logMetrics(epoch_ndx, 'train', trnMetrics_t))   \n",
    "    score = logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "    valid_scores.append(score) \n",
    "    if score <= best_score:\n",
    "        early_stopping_count += 1\n",
    "    best_score = max(score, best_score)\n",
    "    if early_stopping_count==EARLY_STOPPING:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS_LABEL_NDX=0\n",
    "METRICS_PRED_NDX=1\n",
    "METRICS_PRED_P_NDX=2\n",
    "METRICS_LOSS_NDX=3\n",
    "METRICS_SIZE = 4\n",
    "\n",
    "\n",
    "def doTraining(model:torch.nn.Module, optimizer:torch.optim.Optimizer, epoch_ndx:int, train_dl:torch.utils.data.DataLoader)->torch.Tensor:\n",
    "    model.train()\n",
    "    train_dl.dataset.shuffleSamples()\n",
    "    trnMetrics_g = torch.zeros(\n",
    "        METRICS_SIZE,\n",
    "        len(train_dl.dataset),\n",
    "        device='cuda',\n",
    "    )\n",
    "    bar = tqdm(train_dl)\n",
    "    for batch_ndx, batch_tup in enumerate(bar):\n",
    "        optimizer.zero_grad()\n",
    "        loss_var = computeBatchLoss(\n",
    "            model,\n",
    "            batch_ndx,\n",
    "            batch_tup,\n",
    "            train_dl.batch_size,\n",
    "            trnMetrics_g,\n",
    "            augment=True\n",
    "        )\n",
    "        loss_var.backward()\n",
    "        optimizer.step()\n",
    "        bar.set_description('loss: %.5f' % loss_var.item())\n",
    "    return trnMetrics_g.to('cpu')\n",
    "\n",
    "\n",
    "def doValidation(model:torch.nn.Module, epoch_ndx:int, val_dl:torch.utils.data.DataLoader)->torch.Tensor:\n",
    "    bar = tqdm(val_dl)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        valMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(val_dl.dataset),\n",
    "            device='cuda',\n",
    "        )\n",
    "\n",
    "        for batch_ndx, batch_tup in enumerate(bar):\n",
    "            computeBatchLoss(\n",
    "                model,\n",
    "                batch_ndx,\n",
    "                batch_tup,\n",
    "                val_dl.batch_size,\n",
    "                valMetrics_g,\n",
    "                augment=False\n",
    "            )\n",
    "\n",
    "    return valMetrics_g.to('cpu')\n",
    "\n",
    "def computeBatchLoss(model:torch.nn.Module, batch_ndx:int, batch_tup:tuple, batch_size:int, metrics_g:torch.Tensor,\n",
    "                         augment:bool=True):\n",
    "    input_t, label_t, index_t, _series_list, _center_list = batch_tup\n",
    "\n",
    "    input_g = input_t.to('cuda', non_blocking=True)\n",
    "    label_g = label_t.to('cuda', non_blocking=True)\n",
    "    index_g = index_t.to('cuda', non_blocking=True)\n",
    "\n",
    "    logits_g, probability_g = model(input_g)\n",
    "\n",
    "    loss_g = torch.nn.functional.cross_entropy(logits_g, label_g[:, 1],\n",
    "                                            reduction=\"none\")\n",
    "    start_ndx = batch_ndx * batch_size\n",
    "    end_ndx = start_ndx + label_t.size(0)\n",
    "\n",
    "    _, predLabel_g = torch.max(probability_g, dim=1, keepdim=False,\n",
    "                                out=None)\n",
    "\n",
    "    metrics_g[METRICS_LABEL_NDX, start_ndx:end_ndx] = index_g\n",
    "    metrics_g[METRICS_PRED_NDX, start_ndx:end_ndx] = predLabel_g\n",
    "    metrics_g[METRICS_PRED_P_NDX, start_ndx:end_ndx] = probability_g[:,1]\n",
    "    metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = loss_g\n",
    "\n",
    "    return loss_g.mean()\n",
    "\n",
    "def logMetrics(\n",
    "    epoch_ndx,\n",
    "    mode_str,\n",
    "    metrics_t,\n",
    "    classificationThreshold=0.5\n",
    "    ):\n",
    "    pos = 'pos'\n",
    "    neg = 'neg'\n",
    "    negLabel_mask = metrics_t[METRICS_LABEL_NDX] == 0\n",
    "    negPred_mask = metrics_t[METRICS_PRED_NDX] == 0\n",
    "\n",
    "    posLabel_mask = ~negLabel_mask\n",
    "    posPred_mask = ~negPred_mask\n",
    "\n",
    "    neg_count = int(negLabel_mask.sum())\n",
    "    pos_count = int(posLabel_mask.sum())\n",
    "    \n",
    "    neg_correct = int((negLabel_mask & negPred_mask).sum())\n",
    "    pos_correct = int((posLabel_mask & posPred_mask).sum())\n",
    "    trueNeg_count = neg_correct\n",
    "    truePos_count = pos_correct\n",
    "\n",
    "    falsePos_count = neg_count - neg_correct\n",
    "    falseNeg_count = pos_count - pos_correct\n",
    "\n",
    "    metrics_dict = {}\n",
    "    metrics_dict['loss/all'] = metrics_t[METRICS_LOSS_NDX].mean()\n",
    "    metrics_dict['loss/neg'] = metrics_t[METRICS_LOSS_NDX, negLabel_mask].mean()\n",
    "    metrics_dict['loss/pos'] = metrics_t[METRICS_LOSS_NDX, posLabel_mask].mean()\n",
    "    \n",
    "    metrics_dict['correct/all'] = (pos_correct + neg_correct) / metrics_t.shape[1] * 100\n",
    "    metrics_dict['correct/neg'] = (neg_correct) / neg_count * 100\n",
    "    metrics_dict['correct/pos'] = (pos_correct) / pos_count * 100\n",
    "    \n",
    "    precision = metrics_dict['pr/precision'] = truePos_count / np.float64(truePos_count + falsePos_count)\n",
    "    recall    = metrics_dict['pr/recall'] = truePos_count / np.float64(truePos_count + falseNeg_count)\n",
    "\n",
    "    metrics_dict['pr/f1_score'] = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    threshold = torch.linspace(1, 0)\n",
    "    tpr = (metrics_t[None, METRICS_PRED_P_NDX, posLabel_mask] >= threshold[:, None]).sum(1).float() / pos_count\n",
    "    fpr = (metrics_t[None, METRICS_PRED_P_NDX, negLabel_mask] >= threshold[:, None]).sum(1).float() / neg_count\n",
    "    fp_diff = fpr[1:]-fpr[:-1]\n",
    "    tp_avg  = (tpr[1:]+tpr[:-1])/2\n",
    "    auc = (fp_diff * tp_avg).sum()\n",
    "    metrics_dict['auc'] = auc\n",
    "\n",
    "    for key, value in metrics_dict.items():\n",
    "        key = key.replace('pos', pos)\n",
    "        key = key.replace('neg', neg)\n",
    "\n",
    "    score = metrics_dict['auc']\n",
    "\n",
    "    return score\n",
    "\n",
    "def getDataloader(series_uid:str, candidate_info_list:list)->torch.utils.data.DataLoader:\n",
    "    ds = LunaDataset(\n",
    "        val_stride=0,\n",
    "        isValSet_bool=False,\n",
    "        ratio_int=0,\n",
    "        series_uid=series_uid,\n",
    "        candidateInfo_list=candidate_info_list)\n",
    "\n",
    "    dl = torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "    return dl\n",
    "\n",
    "def train_and_validate(train_series_uid:str, train_candidate_info_list:list, valid_series_uid:str, valid_candidate_info_list:list, save_path:str)->torch.nn.Module:\n",
    "    classification_model = LunaModel(in_channels=1, conv_channels=8)\n",
    "    classification_model.to('cuda')\n",
    "    optimizer = torch.optim.SGD(classification_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    train_dl = getDataloader(train_series_uid, train_candidate_info_list)\n",
    "    val_dl   = getDataloader(valid_series_uid, valid_candidate_info_list)\n",
    "    best_score   = 0.0\n",
    "    train_scores = []\n",
    "    valid_scores = []\n",
    "    early_stopping_count = 0\n",
    "    for epoch_ndx in range(1, N_EPOCHS+1):\n",
    "        trnMetrics_t = doTraining(classification_model, optimizer, epoch_ndx, train_dl)\n",
    "        train_scores.append(logMetrics(epoch_ndx, 'train', trnMetrics_t))\n",
    "        valMetrics_t = doValidation(classification_model, epoch_ndx, val_dl)\n",
    "        score = logMetrics(epoch_ndx, 'valid', valMetrics_t)\n",
    "        valid_scores.append(score)\n",
    "        if score <= best_score:\n",
    "            early_stopping_count += 1\n",
    "        else:\n",
    "            torch.save(classification_model.state_dict(), save_path)\n",
    "        best_score = max(score, best_score)\n",
    "        if early_stopping_count == EARLY_STOPPING:\n",
    "            break\n",
    "    print(f'Best Score: {best_score}')\n",
    "    return classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通常のデータを使用した場合\n",
    "classification_model = train_and_validate(\n",
    "    train_series_uid=classification_train_series_uid,\n",
    "    train_candidate_info_list=None, \n",
    "    valid_series_uid=classification_valid_series_uid,\n",
    "    valid_candidate_info_list=None,\n",
    "    save_path='classification_model.pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_info_list = getCandidateInfoList()\n",
    "candidate_info_list = [x for x in candidate_info_list if x.isNodule_bool]\n",
    "train_false_candidate_info_list = getSegmentedCandidateList(segmentation_model=segmentation_model, series_uid=classification_train_series_uid)\n",
    "valid_false_candidate_info_list = getSegmentedCandidateList(segmentation_model=segmentation_model, series_uid=classification_valid_series_uid)\n",
    "classification_train_candidate_info_list = candidate_info_list+train_false_candidate_info_list\n",
    "classification_valid_candidate_info_list = candidate_info_list+valid_false_candidate_info_list\n",
    "shuffle(classification_train_candidate_info_list)\n",
    "shuffle(classification_valid_candidate_info_list)\n",
    "classification_train_candidate_info_list = sorted(classification_train_candidate_info_list, key=attrgetter('series_uid'))\n",
    "classification_valid_candidate_info_list = sorted(classification_valid_candidate_info_list, key=attrgetter('series_uid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# セグメンテーションモデルの出力データを使用した場合\n",
    "mod_classification_model = train_and_validate(\n",
    "    train_series_uid=classification_train_series_uid,\n",
    "    train_candidate_info_list=classification_train_candidate_info_list, \n",
    "    valid_series_uid=classification_valid_series_uid,\n",
    "    valid_candidate_info_list=classification_valid_candidate_info_list,\n",
    "    save_path='mod_classification_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出力されたベストスコア（AUC）を確認すると、セグメンテーションモデルで作成した非結節リストを使用した方がスコアが高いことが確認できます。\n",
    "よって、分類モデルの性能は向上していると判断できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) どのような種類の結節候補が、新しく訓練されたモデルで最も大きな変化を示しますか。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(model_path:str, series_uid:list, candidate_info_list:list)->torch.Tensor:\n",
    "    classification_model = LunaModel(in_channels=1, conv_channels=8)\n",
    "    classification_model.load_state_dict(torch.load(model_path))\n",
    "    classification_model.to('cuda')\n",
    "    dl = getDataloader(series_uid, candidate_info_list)\n",
    "    metrics_t = doValidation(classification_model, 0, dl)\n",
    "    return metrics_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_classification_candidate_info_list = [x for x in classification_valid_candidate_info_list if x.isNodule_bool]\n",
    "original_metrics = pred(model_path='classification_model.pt', \n",
    "                        series_uid=classification_valid_series_uid, \n",
    "                        candidate_info_list=positive_classification_candidate_info_list)\n",
    "mod_metrics = pred(model_path='mod_classification_model.pt', \n",
    "                   series_uid=classification_valid_series_uid, \n",
    "                   candidate_info_list=positive_classification_candidate_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (mod_metrics[2,:]-original_metrics[2,:]).numpy()\n",
    "target_candidate_info_list = [x for x in positive_classification_candidate_info_list if x.series_uid in classification_valid_series_uid]\n",
    "diameters = [x.diameter_mm for x in target_candidate_info_list]\n",
    "data = pd.DataFrame(np.stack((diff, diameters)).T, columns=['diff', 'diameter'])\n",
    "sns.scatterplot(x='diameter', y='diff', data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直径の大きな結節ほど、新しいモデルによって予測をうまく行えていることがわかります。\n",
    "※なお、本解説ではcolabでの実行時間を考慮して、数個のseries_uidをサンプルしてコードを実行しています。\n",
    "　そのため、本来は上記の図をもって結節の大きさと予測値の関係を結論づけることはできません。\n",
    "　実行時間や計算リソースの制約が無い方は、ぜひサンプルするseries_uidの数を増やしてコードを実行してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [4] 今回使用したパディング付き畳み込みは、画像の端の近くで完全な情報が得られません。CTスキャンスライスのエッジ近くのセグメント化されたピクセルとスライスの内側のピクセルの損失を計算してください。2つの間に測定可能な違いはありますか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "違いは存在します。パディングを行わない場合、画像の端のピクセルは中心近くのピクセルと比較して、畳み込みに利用される頻度が少なくなります。\n",
    "したがって、画像端のピクセルはパディングなしの場合、中心のピクセルと比較して情報の損失が大きくなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5] 32 × 48 × 48のパッチを重ね合わせて、CT全体に対して分類器を実行してみてください。これはセグメンテーションアプローチと比較してどうなりますか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "省略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
