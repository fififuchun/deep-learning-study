{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] torchvisionを使ってランダムにクロップされたデータを実装してください。\n",
    "※ ここではGoogle Colaraboratoryでの実行を想定しています。\n",
    "\n",
    "※ Google Colaraboratoryでbashコマンドを実行するには、命令の前に!をつけます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【a】クロップ後の画像は、クロップされていない元の画像とどのように異なるのか確認してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答\n",
    "# クロップ後の画像は引数で指定したサイズで元の画像からランダムに切り取られます(指定するサイズは元のサイズ以下にする)。\n",
    "# したがって、元画像とはサイズと画像に写っている部分が異なります。\n",
    "# ランダムクロップはtransforms.RandomCropを使用することで実現できます。\n",
    "# transforms.RandomCropを用いると引数に指定したサイズで（例：サイズ24の場合は24×24の画像）が元の画像からランダムに切り取られた画像が得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12dd61f10>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHubのリポジトリから実行の場合\n",
    "data_path = \"../data-unversioned/p1ch7/\"\n",
    "\n",
    "# Google Colaboratoryの場合\n",
    "# /data/p1ch4/tabular-wine/winequality-white.csvを選択・アップロードしてください\n",
    "#from google.colab import files\n",
    "#uploade = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True) # <1>\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True) # <2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行するたびに次のセルの実行結果がランダムに変わることが確認できます\n",
    "transformed_cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transforms.RandomCrop(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_t, _ = transformed_cifar10[99]\n",
    "\n",
    "plt.imshow(img_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【b】同じ画像を2回クロップするとどうなるか確認してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答\n",
    "# 1回目のcropで得られた画像から、さらにランダムに切り取られた画像が得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_crop = transforms.RandomCrop(16)\n",
    "img_t = random_crop(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_t)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【c】ランダムにクロップされた画像を使った訓練結果を確認してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答\n",
    "# 今回のモデルの場合、cropした画像で訓練すると、精度は下がります。\n",
    "# これは全結合のモデルではクロップによる画像の空間的変化を認識できないためです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 30\n",
    "\n",
    "transform_crop = transforms.Compose([\n",
    "        transforms.RandomCrop(crop_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "transform_no_crop = transforms.Compose([       \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transform_no_crop)\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transform_no_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10 \n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.632668\n",
      "Epoch: 1, Loss: 0.583662\n",
      "Epoch: 2, Loss: 0.510644\n",
      "Epoch: 3, Loss: 0.522492\n",
      "Epoch: 4, Loss: 0.572710\n",
      "Epoch: 5, Loss: 0.670232\n",
      "Epoch: 6, Loss: 0.651678\n",
      "Epoch: 7, Loss: 0.260835\n",
      "Epoch: 8, Loss: 0.649594\n",
      "Epoch: 9, Loss: 0.412848\n",
      "Epoch: 10, Loss: 0.662059\n",
      "Epoch: 11, Loss: 0.474887\n",
      "Epoch: 12, Loss: 0.350090\n",
      "Epoch: 13, Loss: 0.310838\n",
      "Epoch: 14, Loss: 0.929018\n",
      "Epoch: 15, Loss: 0.274175\n",
      "Epoch: 16, Loss: 0.479683\n",
      "Epoch: 17, Loss: 0.287188\n",
      "Epoch: 18, Loss: 0.373591\n",
      "Epoch: 19, Loss: 0.499466\n",
      "Epoch: 20, Loss: 0.305920\n",
      "Epoch: 21, Loss: 0.406155\n",
      "Epoch: 22, Loss: 0.432189\n",
      "Epoch: 23, Loss: 0.581197\n",
      "Epoch: 24, Loss: 0.459233\n",
      "Epoch: 25, Loss: 0.435899\n",
      "Epoch: 26, Loss: 0.535097\n",
      "Epoch: 27, Loss: 0.391015\n",
      "Epoch: 28, Loss: 0.241818\n",
      "Epoch: 29, Loss: 0.437882\n",
      "Epoch: 30, Loss: 0.345004\n",
      "Epoch: 31, Loss: 0.501271\n",
      "Epoch: 32, Loss: 0.579021\n",
      "Epoch: 33, Loss: 0.542437\n",
      "Epoch: 34, Loss: 0.450785\n",
      "Epoch: 35, Loss: 0.390124\n",
      "Epoch: 36, Loss: 0.505412\n",
      "Epoch: 37, Loss: 0.656195\n",
      "Epoch: 38, Loss: 0.604508\n",
      "Epoch: 39, Loss: 0.593516\n",
      "Epoch: 40, Loss: 0.514931\n",
      "Epoch: 41, Loss: 0.432960\n",
      "Epoch: 42, Loss: 0.512649\n",
      "Epoch: 43, Loss: 0.634816\n",
      "Epoch: 44, Loss: 0.313197\n",
      "Epoch: 45, Loss: 0.760799\n",
      "Epoch: 46, Loss: 0.322675\n",
      "Epoch: 47, Loss: 0.390263\n",
      "Epoch: 48, Loss: 0.355759\n",
      "Epoch: 49, Loss: 0.597017\n",
      "Epoch: 50, Loss: 0.656835\n",
      "Epoch: 51, Loss: 0.421180\n",
      "Epoch: 52, Loss: 0.272409\n",
      "Epoch: 53, Loss: 0.348669\n",
      "Epoch: 54, Loss: 0.847906\n",
      "Epoch: 55, Loss: 0.577339\n",
      "Epoch: 56, Loss: 0.258456\n",
      "Epoch: 57, Loss: 0.318334\n",
      "Epoch: 58, Loss: 0.630863\n",
      "Epoch: 59, Loss: 0.260645\n",
      "Epoch: 60, Loss: 0.510950\n",
      "Epoch: 61, Loss: 0.364905\n",
      "Epoch: 62, Loss: 0.441027\n",
      "Epoch: 63, Loss: 0.136111\n",
      "Epoch: 64, Loss: 0.346525\n",
      "Epoch: 65, Loss: 0.349466\n",
      "Epoch: 66, Loss: 0.633202\n",
      "Epoch: 67, Loss: 0.319315\n",
      "Epoch: 68, Loss: 0.166419\n",
      "Epoch: 69, Loss: 0.318420\n",
      "Epoch: 70, Loss: 0.406870\n",
      "Epoch: 71, Loss: 0.640828\n",
      "Epoch: 72, Loss: 0.426609\n",
      "Epoch: 73, Loss: 0.505127\n",
      "Epoch: 74, Loss: 0.524943\n",
      "Epoch: 75, Loss: 0.308573\n",
      "Epoch: 76, Loss: 0.513980\n",
      "Epoch: 77, Loss: 0.517717\n",
      "Epoch: 78, Loss: 0.383469\n",
      "Epoch: 79, Loss: 0.315950\n",
      "Epoch: 80, Loss: 0.415638\n",
      "Epoch: 81, Loss: 0.514403\n",
      "Epoch: 82, Loss: 0.241211\n",
      "Epoch: 83, Loss: 0.260677\n",
      "Epoch: 84, Loss: 0.311338\n",
      "Epoch: 85, Loss: 0.274925\n",
      "Epoch: 86, Loss: 0.248918\n",
      "Epoch: 87, Loss: 0.338333\n",
      "Epoch: 88, Loss: 0.521916\n",
      "Epoch: 89, Loss: 0.297568\n",
      "Epoch: 90, Loss: 0.216847\n",
      "Epoch: 91, Loss: 0.252971\n",
      "Epoch: 92, Loss: 0.536784\n",
      "Epoch: 93, Loss: 0.320327\n",
      "Epoch: 94, Loss: 0.473559\n",
      "Epoch: 95, Loss: 0.314498\n",
      "Epoch: 96, Loss: 0.294574\n",
      "Epoch: 97, Loss: 0.129971\n",
      "Epoch: 98, Loss: 0.444935\n",
      "Epoch: 99, Loss: 0.241321\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "input_size = crop_size * crop_size *3\n",
    "#input_size = 3072 # cropなし\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 2))\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.843300\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.812000\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] 損失関数を切り替えてください（可能であればMSE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【a】訓練の振る舞いが変わるか確認してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答\n",
    "# lossをMSEに変えた場合、訓練の振る舞いは変わります。\n",
    "# 確率ではなくラベルを数値に見立て、回帰問題として学習しています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/goto/.anyenv/envs/pyenv/versions/3.7.4/lib/python3.7/site-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.399389\n",
      "Epoch: 1, Loss: 0.308467\n",
      "Epoch: 2, Loss: 0.251620\n",
      "Epoch: 3, Loss: 0.250273\n",
      "Epoch: 4, Loss: 0.242244\n",
      "Epoch: 5, Loss: 0.253378\n",
      "Epoch: 6, Loss: 0.247346\n",
      "Epoch: 7, Loss: 0.256292\n",
      "Epoch: 8, Loss: 0.250403\n",
      "Epoch: 9, Loss: 0.239607\n",
      "Epoch: 10, Loss: 0.247572\n",
      "Epoch: 11, Loss: 0.257202\n",
      "Epoch: 12, Loss: 0.248193\n",
      "Epoch: 13, Loss: 0.251038\n",
      "Epoch: 14, Loss: 0.250824\n",
      "Epoch: 15, Loss: 0.253099\n",
      "Epoch: 16, Loss: 0.253059\n",
      "Epoch: 17, Loss: 0.249603\n",
      "Epoch: 18, Loss: 0.252977\n",
      "Epoch: 19, Loss: 0.250052\n",
      "Epoch: 20, Loss: 0.250553\n",
      "Epoch: 21, Loss: 0.251027\n",
      "Epoch: 22, Loss: 0.250954\n",
      "Epoch: 23, Loss: 0.253104\n",
      "Epoch: 24, Loss: 0.250690\n",
      "Epoch: 25, Loss: 0.250250\n",
      "Epoch: 26, Loss: 0.250044\n",
      "Epoch: 27, Loss: 0.250358\n",
      "Epoch: 28, Loss: 0.250387\n",
      "Epoch: 29, Loss: 0.250856\n",
      "Epoch: 30, Loss: 0.252075\n",
      "Epoch: 31, Loss: 0.249426\n",
      "Epoch: 32, Loss: 0.250320\n",
      "Epoch: 33, Loss: 0.250944\n",
      "Epoch: 34, Loss: 0.250788\n",
      "Epoch: 35, Loss: 0.247655\n",
      "Epoch: 36, Loss: 0.248636\n",
      "Epoch: 37, Loss: 0.251530\n",
      "Epoch: 38, Loss: 0.249725\n",
      "Epoch: 39, Loss: 0.250252\n",
      "Epoch: 40, Loss: 0.248842\n",
      "Epoch: 41, Loss: 0.249148\n",
      "Epoch: 42, Loss: 0.250732\n",
      "Epoch: 43, Loss: 0.250398\n",
      "Epoch: 44, Loss: 0.250244\n",
      "Epoch: 45, Loss: 0.249885\n",
      "Epoch: 46, Loss: 0.249774\n",
      "Epoch: 47, Loss: 0.250946\n",
      "Epoch: 48, Loss: 0.250982\n",
      "Epoch: 49, Loss: 0.250187\n",
      "Epoch: 50, Loss: 0.249796\n",
      "Epoch: 51, Loss: 0.247662\n",
      "Epoch: 52, Loss: 0.251063\n",
      "Epoch: 53, Loss: 0.250185\n",
      "Epoch: 54, Loss: 0.252220\n",
      "Epoch: 55, Loss: 0.252395\n",
      "Epoch: 56, Loss: 0.247495\n",
      "Epoch: 57, Loss: 0.251779\n",
      "Epoch: 58, Loss: 0.254785\n",
      "Epoch: 59, Loss: 0.252036\n",
      "Epoch: 60, Loss: 0.249568\n",
      "Epoch: 61, Loss: 0.250197\n",
      "Epoch: 62, Loss: 0.250318\n",
      "Epoch: 63, Loss: 0.250082\n",
      "Epoch: 64, Loss: 0.250900\n",
      "Epoch: 65, Loss: 0.250863\n",
      "Epoch: 66, Loss: 0.250340\n",
      "Epoch: 67, Loss: 0.248852\n",
      "Epoch: 68, Loss: 0.249890\n",
      "Epoch: 69, Loss: 0.248719\n",
      "Epoch: 70, Loss: 0.250268\n",
      "Epoch: 71, Loss: 0.250199\n",
      "Epoch: 72, Loss: 0.248775\n",
      "Epoch: 73, Loss: 0.250377\n",
      "Epoch: 74, Loss: 0.249347\n",
      "Epoch: 75, Loss: 0.250549\n",
      "Epoch: 76, Loss: 0.249769\n",
      "Epoch: 77, Loss: 0.252437\n",
      "Epoch: 78, Loss: 0.250253\n",
      "Epoch: 79, Loss: 0.250277\n",
      "Epoch: 80, Loss: 0.250916\n",
      "Epoch: 81, Loss: 0.250172\n",
      "Epoch: 82, Loss: 0.250445\n",
      "Epoch: 83, Loss: 0.249875\n",
      "Epoch: 84, Loss: 0.250410\n",
      "Epoch: 85, Loss: 0.249919\n",
      "Epoch: 86, Loss: 0.249472\n",
      "Epoch: 87, Loss: 0.250268\n",
      "Epoch: 88, Loss: 0.249444\n",
      "Epoch: 89, Loss: 0.250149\n",
      "Epoch: 90, Loss: 0.250694\n",
      "Epoch: 91, Loss: 0.249754\n",
      "Epoch: 92, Loss: 0.250269\n",
      "Epoch: 93, Loss: 0.249452\n",
      "Epoch: 94, Loss: 0.250167\n",
      "Epoch: 95, Loss: 0.250183\n",
      "Epoch: 96, Loss: 0.250534\n",
      "Epoch: 97, Loss: 0.251598\n",
      "Epoch: 98, Loss: 0.250304\n",
      "Epoch: 99, Loss: 0.249434\n"
     ]
    }
   ],
   "source": [
    "input_size = 3072 # cropなし\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(input_size, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            )\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels.float())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.606100\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))        \n",
    "        outputs[outputs > 0.5] = int(1)\n",
    "        outputs[outputs < 0.5] = int(0)\n",
    "        outputs = outputs.squeeze()     \n",
    "        total += labels.shape[0]\n",
    "        correct += int((outputs == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] ネットワークの容量を十分に減らすことで、過学習を停止することは可能ですか？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【a】このとき、モデルは検証セットでどのように機能しますか？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答\n",
    "# 隠れ層を1層減らし、ネットワークの大きさを縮小することで過学習の停止（抑制）は可能です\n",
    "# この時、検証セットでの精度は過学習が抑制されたことにより、モデルを小さくする前より若干ですが改善します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_no_crop = transforms.Compose([       \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ])\n",
    "\n",
    "\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform=transform_no_crop)\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transform_no_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10 \n",
    "          if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, train_loss: 0.610557, val_loss: 0.566712\n",
      "Epoch: 10, train_loss: 0.484836, val_loss: 0.479519\n",
      "Epoch: 20, train_loss: 0.461390, val_loss: 0.460741\n",
      "Epoch: 30, train_loss: 0.444578, val_loss: 0.448938\n",
      "Epoch: 40, train_loss: 0.430208, val_loss: 0.440196\n",
      "Epoch: 50, train_loss: 0.416957, val_loss: 0.435150\n",
      "Epoch: 60, train_loss: 0.403863, val_loss: 0.439122\n",
      "Epoch: 70, train_loss: 0.388677, val_loss: 0.427953\n",
      "Epoch: 80, train_loss: 0.372876, val_loss: 0.423397\n",
      "Epoch: 90, train_loss: 0.358055, val_loss: 0.428216\n",
      "Epoch: 100, train_loss: 0.340593, val_loss: 0.421416\n",
      "Epoch: 110, train_loss: 0.321357, val_loss: 0.447057\n",
      "Epoch: 120, train_loss: 0.299042, val_loss: 0.425308\n",
      "Epoch: 130, train_loss: 0.275910, val_loss: 0.429433\n",
      "Epoch: 140, train_loss: 0.249123, val_loss: 0.437562\n"
     ]
    }
   ],
   "source": [
    "input_size = 3072 # cropなし\n",
    "\n",
    "# 元のモデル\n",
    "# model = nn.Sequential(\n",
    "#             nn.Linear(input_size, 1024),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(512, 128),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(128, 2),\n",
    "#             )\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),            \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 2))\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 150\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    avg_train_loss = train_loss /len(train_loader)    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: %d, train_loss: %f, val_loss: %f\" % (epoch, avg_train_loss, avg_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.912400\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.798500\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        \n",
    "print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}